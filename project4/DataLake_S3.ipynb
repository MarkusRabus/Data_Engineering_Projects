{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Engineering Project 4: Data Lakes with Spark\n",
    "\n",
    "Introduction\n",
    "\n",
    "A music streaming startup, Sparkify, has grown their user base and song database even more and want to move their data warehouse to a data lake. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.\n",
    "\n",
    "In this notebook we test an ETL pipeline that extracts a local copy of the data, processes them using Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField as Fld, DoubleType as Dbl, \\\n",
    "        StringType as Str, IntegerType as Int, DateType as Date, StringType as Str, \\\n",
    "        LongType as Lgt, TimestampType as Tme\n",
    "from pyspark.sql.functions import *\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Lines below are commented out, because we are using a local copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#config = configparser.ConfigParser()\n",
    "#config.read_file(open('aws/credentials.cfg'))\n",
    "#os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "#os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Upack the zip files with the data and save it in the `data` folder.\n",
    "\n",
    "`song-data.zip` has the following path structure: `song-data/*/*/*/*.json`\n",
    "\n",
    "`log-data.zip` has the following path structure: `*.json`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "with ZipFile('data/song-data.zip','r') as zip_file:\n",
    "    zip_file.extractall('data/')\n",
    "with ZipFile('data/log-data.zip','r') as zip_file:\n",
    "    zip_file.extractall('data/log_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Create the schema of the `json` files we are going to read. We create one schema for the `song-data` and one for the `log-data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songSchema = StructType([\n",
    "    Fld(\"artist_id\",       Str()),\n",
    "    Fld(\"artist_latitude\", Dbl()),\n",
    "    Fld(\"artist_location\", Str()),\n",
    "    Fld(\"artist_longitude\",Dbl()),\n",
    "    Fld(\"artist_name\",     Str()),\n",
    "    Fld(\"duration\",        Dbl()),\n",
    "    Fld(\"num_songs\",       Int()),\n",
    "    Fld(\"song_id\",         Str()),\n",
    "    Fld(\"title\",           Str()),\n",
    "    Fld(\"year\",            Int())\n",
    "    ])\n",
    "\n",
    "logschema = StructType([\n",
    "    Fld('artist',          Str()),\n",
    "    Fld('auth',            Str()),\n",
    "    Fld('firstName',       Str()),\n",
    "    Fld('gender',          Str()),\n",
    "    Fld('itemInSession',   Int()),\n",
    "    Fld('lastName',        Str()),\n",
    "    Fld('length',          Dbl()),\n",
    "    Fld('level',           Str()),\n",
    "    Fld('location',        Str()),\n",
    "    Fld('method',          Str()),\n",
    "    Fld('page',            Str()),\n",
    "    Fld('registration',    Dbl()),\n",
    "    Fld('sessionId',       Int()),\n",
    "    Fld('song',            Str()),\n",
    "    Fld('status',          Int()),\n",
    "    Fld('ts',              Lgt()),\n",
    "    Fld('userAgent',       Str()),\n",
    "    Fld('userId',          Int())\n",
    "    ]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We load the data, S3 download has been commented out, because we use the local copy of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#uncomment if S3\n",
    "#input_data = \"s3a://udacity-dend/\"\n",
    "#song_data = os.path.join(input_data, \"song_data/A/A/A/*.json\")\n",
    "#uncomment if local\n",
    "input_data = \"data/\"\n",
    "song_data = os.path.join(input_data, \"song_data/*/*/*/*.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Load data in spark data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_song = spark.read.json(song_data, schema=songSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We check the schema and print the first row, to check if the data has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: integer (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(artist_id='ARDR4AC1187FB371A1', artist_latitude=None, artist_location='', artist_longitude=None, artist_name='Montserrat Caball√©;Placido Domingo;Vicente Sardinero;Judith Blegen;Sherrill Milnes;Georg Solti', duration=511.16363, num_songs=1, song_id='SOBAYLL12A8C138AF9', title='Sono andati? Fingevo di dormire', year=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_song.printSchema()\n",
    "df_song.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Next we load the `log-data` into a spark data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get filepath to song data file\n",
    "#log_data = os.path.join(input_data, \"log_data/2018/11/*.json\")\n",
    "log_data = os.path.join(input_data, \"log_data/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_log = spark.read.json(log_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Check that we have loaded the `log-data` correctly into the spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(artist='Harmonia', auth='Logged In', firstName='Ryan', gender='M', itemInSession=0, lastName='Smith', length=655.77751, level='free', location='San Jose-Sunnyvale-Santa Clara, CA', method='PUT', page='NextSong', registration=1541016707796.0, sessionId=583, song='Sehr kosmisch', status=200, ts=1542241826796, userAgent='\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/36.0.1985.125 Chrome/36.0.1985.125 Safari/537.36\"', userId='26')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log.printSchema()\n",
    "df_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We join both data frames, based on `artist_name` and `song_title`. We also create an additional column with the `ts`-column converted into timestamp type column. Later we can use the `start_time`-column to create the time table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_log_join_song = df_log.join(df_song, (df_song.artist_name == df_log.artist) & (df_song.title == df_log.song))\n",
    "df_log_join_song = df_log_join_song.withColumn( 'start_time', (round( col('ts')/1000 )).cast(Tme()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: double, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string, artist_id: string, artist_latitude: double, artist_location: string, artist_longitude: double, artist_name: string, duration: double, num_songs: int, song_id: string, title: string, year: int, start_time: timestamp]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log_join_song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Create the `songs_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songs_table = df_log_join_song.select('song_id', 'title', 'artist_id', 'year', 'duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Check the `songs_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(song_id='SOZCTXZ12AB0182364', title='Setanta matins', artist_id='AR5KOSW1187FB35FF4', year=0, duration=269.58322)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Create the `user_table`. We use pyspark alias to rename the respective column in accordance with the project instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_table = df_log_join_song.select(col('userId').alias('user_id'), \n",
    "                                     col('firstName').alias('first_name'), \n",
    "                                     col('lastName').alias('last_name'), \n",
    "                                     col('gender'), \n",
    "                                     col('level'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Check the `user_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(user_id='15', first_name='Lily', last_name='Koch', gender='F', level='paid')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Create the `artist_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "artist_table = df_log_join_song.select(col('artist_id'), \n",
    "                                     col('artist_name').alias('name'), \n",
    "                                     col('artist_location').alias('location'), \n",
    "                                     col('artist_latitude').alias('latitude'),\n",
    "                                     col('artist_longitude').alias('longitude'),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Check the `artist_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist_id='AR5KOSW1187FB35FF4', name='Elena', location='Dubai UAE', latitude=49.80388, longitude=15.47491)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artist_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Create the `time_table`. We use the pyspark functions to convert the time stamp `start_time`. We also make sure that the time stamp is unique `dropDuplicates`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_table = df_log_join_song.select(col('start_time'), \n",
    "                                     hour(df_log_join_song.start_time).alias('hour'), \n",
    "                                     dayofmonth(df_log_join_song.start_time).alias('day'), \n",
    "                                     weekofyear(df_log_join_song.start_time).alias('week'),\n",
    "                                     month(df_log_join_song.start_time).alias('month'),\n",
    "                                     year(df_log_join_song.start_time).alias('year'),\n",
    "                                     dayofweek(df_log_join_song.start_time).alias('weekday')).dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Check the `time-table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(start_time=datetime.datetime(2018, 11, 21, 21, 56, 48), hour=21, day=21, week=47, month=11, year=2018, weekday=4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Create `songplays_table`, rename column names and filter to include only records with page `NextSong`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_table = df_log_join_song.select(col('start_time'), \n",
    "                                     col('userId').alias('user_id'), \n",
    "                                     col('level'), \n",
    "                                     col('song_id'), \n",
    "                                     col('artist_id'),\n",
    "                                     col('sessionId').alias('session_id'),\n",
    "                                     col('location'),\n",
    "                                     col('userAgent').alias('user_agent')).filter(\"page = 'NextSong'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Check `songplays_table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(start_time=datetime.datetime(2018, 11, 21, 21, 56, 48), user_id='15', level='paid', song_id='SOZCTXZ12AB0182364', artist_id='AR5KOSW1187FB35FF4', session_id=818, location='Chicago-Naperville-Elgin, IL-IN-WI', user_agent='\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/36.0.1985.125 Chrome/36.0.1985.125 Safari/537.36\"')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songplays_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
