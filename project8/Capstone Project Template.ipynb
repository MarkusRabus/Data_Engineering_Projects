{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "In this project we will collect data sets and create tables which will allow to assess the influence of solar flares on weathern patterns on Earth. An investigation showed that small fluctuations in spot coverage can influence the climate.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import os, glob\n",
    "import shutil\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In order to evaluate the influence of sunspot coverage on earth weather patter we combined data from two sources. For one, we need the time series for the sun spot coverage and the other data which logs the weather.\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Two data sets are used:\n",
    "\n",
    "* Sunspots ([link](https://www.kaggle.com/robervalt/sunspots)): This dataset is a csv file with the date and the montly mean total sunspot number as obtained by the Database from SIDC - Solar Influences Data Analysis Center - the solar physics research department of the Royal Observatory of Belgium.\n",
    "\n",
    "* NOAA GSOD ([link](https://data.noaa.gov/dataset/dataset/global-surface-summary-of-the-day-gsod)): This is a public data set from the National Oceanic and Atmospheric Administration (NOAA). This data set is very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data exploration:\n",
    "\n",
    "We start by understanding the content of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create spark session\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Comparing solar flares with global weathern pattern\")\\\n",
    "        .enableHiveSupport().getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Read sunspot csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#csvFile = 'hessi.solar.flare.2002to2016.csv'\n",
    "csvFile = 'Sunspots.csv'\n",
    "dfSpots = spark.read.csv(csvFile, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Print the schema, show the size of the data and the date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Monthly Mean Total Sunspot Number: double (nullable = true)\n",
      "\n",
      "3252\n",
      "Row(_c0=0, Date=datetime.datetime(1749, 1, 31, 0, 0), Monthly Mean Total Sunspot Number=96.7)\n",
      "+-------------------+\n",
      "|          min(Date)|\n",
      "+-------------------+\n",
      "|1749-01-31 00:00:00|\n",
      "+-------------------+\n",
      "\n",
      "+-------------------+\n",
      "|          max(Date)|\n",
      "+-------------------+\n",
      "|2019-12-31 00:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSpots.printSchema()\n",
    "print(dfSpots.count())\n",
    "print(dfSpots.head())\n",
    "dfSpots.select([min(\"Date\")]).show()\n",
    "dfSpots.select([max(\"Date\")]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "NOAA data is available as `tar.gz` files for each year. We start by exploring only one year. Each `tar.gz` file has several csv files. As an example we start by showing just one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATION: long (nullable = true)\n",
      " |-- DATE: timestamp (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- TEMP: double (nullable = true)\n",
      " |-- TEMP_ATTRIBUTES: double (nullable = true)\n",
      " |-- DEWP: double (nullable = true)\n",
      " |-- DEWP_ATTRIBUTES: double (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      " |-- SLP_ATTRIBUTES: double (nullable = true)\n",
      " |-- STP: double (nullable = true)\n",
      " |-- STP_ATTRIBUTES: double (nullable = true)\n",
      " |-- VISIB: double (nullable = true)\n",
      " |-- VISIB_ATTRIBUTES: double (nullable = true)\n",
      " |-- WDSP: double (nullable = true)\n",
      " |-- WDSP_ATTRIBUTES: double (nullable = true)\n",
      " |-- MXSPD: double (nullable = true)\n",
      " |-- GUST: double (nullable = true)\n",
      " |-- MAX: double (nullable = true)\n",
      " |-- MAX_ATTRIBUTES: string (nullable = true)\n",
      " |-- MIN: double (nullable = true)\n",
      " |-- MIN_ATTRIBUTES: string (nullable = true)\n",
      " |-- PRCP: double (nullable = true)\n",
      " |-- PRCP_ATTRIBUTES: string (nullable = true)\n",
      " |-- SNDP: double (nullable = true)\n",
      " |-- FRSHTT: integer (nullable = true)\n",
      "\n",
      "358\n",
      "Row(STATION=68491099999, DATE=datetime.datetime(2016, 1, 1, 0, 0), LATITUDE=-28.2, LONGITUDE=32.4166666, ELEVATION=9.0, NAME='CHARTERS CREEK, SF', TEMP=80.6, TEMP_ATTRIBUTES=8.0, DEWP=66.5, DEWP_ATTRIBUTES=8.0, SLP=1014.6, SLP_ATTRIBUTES=8.0, STP=13.7, STP_ATTRIBUTES=8.0, VISIB=999.9, VISIB_ATTRIBUTES=0.0, WDSP=7.5, WDSP_ATTRIBUTES=8.0, MXSPD=12.0, GUST=999.9, MAX=89.8, MAX_ATTRIBUTES=' ', MIN=73.9, MIN_ATTRIBUTES=' ', PRCP=0.0, PRCP_ATTRIBUTES='I', SNDP=999.9, FRSHTT=0)\n"
     ]
    }
   ],
   "source": [
    "#create a temporary directory where the data will be extracted and explored\n",
    "with tempfile.TemporaryDirectory() as temp_directory:\n",
    "    #unpack to year 2016 data to temporary directory\n",
    "    shutil.unpack_archive('/home/workspace/NOAA_data/2016.tar.gz', temp_directory)\n",
    "    #get a list of all csv files\n",
    "    csvFiles = glob.glob( os.path.join(temp_directory,'*.csv') )\n",
    "    #read the first csv file and print some information\n",
    "    dfNOAA = spark.read.csv(csvFiles[0], header=True, inferSchema=True)\n",
    "    dfNOAA.printSchema()\n",
    "    print(dfNOAA.count())\n",
    "    print(dfNOAA.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "\n",
    "***Columns of interest for NOAA data:***\n",
    "\n",
    "* STATION: Station number (WMO/DATSAV3 number) for the location\n",
    "\n",
    "* DATE: Date of reading\n",
    "\n",
    "* LATITUDE: Station latitude\n",
    "\n",
    "* LONGITUDE: Station longitude\n",
    "\n",
    "* ELEVATION: Station elevation\n",
    "\n",
    "* NAME: Station name\n",
    "\n",
    "* TEMP: Temperature reading\n",
    "\n",
    "* DEWP: Dew Point reading\n",
    "\n",
    "* SLP: Mean sea level pressure for the day in millibars to tenths. Missing = 9999.9\n",
    "\n",
    "* STP: Mean station pressure for the day in millibars to tenths. Missing = 9999.9\n",
    "\n",
    "* VISIB: Mean visibility for the day in miles to tenths. Missing = 999.9\n",
    "\n",
    "* WDSP: Mean wind speed for the day in knots to tenths. Missing = 999.9\n",
    "\n",
    "* MXSPD: Maximum sustained wind speed reported for the day in knots to tenths. Missing = 999.9\n",
    "\n",
    "* GUST: Maximum wind gust reported for the day in knots to tenths. Missing = 999.9\n",
    "\n",
    "* MAX: Maximum temperature reported during the day in Fahrenheit to tenths--time of max temp report varies by country and region.\n",
    "\n",
    "* MIN: Minimum temperature reported during the day in Fahrenheit to tenths--time of min temp report varies by country and region.\n",
    "\n",
    "* PRCP: Total precipitation (rain and/or melted snow) reported during the day in inches and hundredths.\n",
    "\n",
    "\n",
    "***Columns of interest for Sun Spot data:***\n",
    "\n",
    "* _c0: Idenfier\n",
    "* Date: Date\n",
    "* Monthly Mean Total Sunspot Number\n",
    "\n",
    "#### Cleaning Steps\n",
    "\n",
    "For the Sunspot data we only drop duplicate entries and separete date and time column.\n",
    "\n",
    "For the NOAA data we select only columns of interestes, we drop duplicates **and** filter missing values. We iterate over all `tar.gz` files and read all csv files in to a spark dataframe. We will save our staged result in a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- spotId: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- SunspotNumber: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3252"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# READ sunspot csv file, drop duplicates and add columns dayofmonth, month and year.\n",
    "csvFile = 'Sunspots.csv'\n",
    "dfSpots = spark.read.csv(csvFile, header=True, inferSchema=True)\n",
    "\n",
    "spotsTable = dfSpots.select(col('_c0').alias('spotId'), \n",
    "                            date_format(dfSpots[\"Date\"], 'yyyy-MM-dd').alias('Date'),\n",
    "                            date_format(dfSpots[\"Date\"], 'h:m:s a').alias('Time'),\n",
    "                            col('Monthly Mean Total Sunspot Number').alias('SunspotNumber'),)\\\n",
    "                            .dropDuplicates()\n",
    "# Show schema for verfication propuses.\n",
    "spotsTable.printSchema()\n",
    "# Show number of counts. This table actually has no duplicates.\n",
    "spotsTable.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(spotId=284, Date='1772-09-30', Time='12:0:0 AM', SunspotNumber=84.2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotsTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# READ NOAA data.\n",
    "noaaFiles = glob.glob('NOAA_data/*.tar.gz') # List of all tar.gz files\n",
    "for tarfile in noaaFiles[0:3]:\n",
    "    with tempfile.TemporaryDirectory() as temp_directory:\n",
    "        #unpack\n",
    "        shutil.unpack_archive(tarfile, temp_directory)\n",
    "        csvFiles = glob.glob( os.path.join(temp_directory,'*.csv') )\n",
    "        #read all csv files into spark dataframe\n",
    "        dfNOAA = spark.read.csv(csvFiles, header=True, inferSchema=True)\n",
    "        #clean up the data\n",
    "        noaaTable = dfNOAA.select('STATION', date_format(dfNOAA[\"DATE\"], 'yyyy-MM-dd').alias('Date'),\n",
    "                                date_format(dfNOAA[\"DATE\"], 'h:m:s a').alias('Time'),\n",
    "                                'LATITUDE', 'LONGITUDE', 'ELEVATION',\n",
    "                                'NAME', 'TEMP', 'DEWP', 'SLP', 'STP', 'VISIB', 'WDSP', 'MXSPD', \n",
    "                                'GUST', 'MAX', 'MIN', 'PRCP').dropDuplicates()\\\n",
    "                                .filter(\"SLP != 9999.9\")\\\n",
    "                                .filter(\"STP != 9999.9\")\\\n",
    "                                .filter(\"VISIB != 999.9\")\\\n",
    "                                .filter(\"WDSP != 999.9\")\\\n",
    "                                .filter(\"MXSPD != 999.9\")\\\n",
    "                                .filter(\"GUST != 999.9\")\n",
    "        #Append the data to a parquet file\n",
    "        noaaTable.write.mode('append').parquet(\"NOAA_data.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Check that parquet file has been created and the schema is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "noaaParquet = spark.read \\\n",
    "                .format('parquet') \\\n",
    "                .load('NOAA_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATION: long (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- TEMP: double (nullable = true)\n",
      " |-- DEWP: double (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      " |-- STP: double (nullable = true)\n",
      " |-- VISIB: double (nullable = true)\n",
      " |-- WDSP: double (nullable = true)\n",
      " |-- MXSPD: double (nullable = true)\n",
      " |-- GUST: double (nullable = true)\n",
      " |-- MAX: double (nullable = true)\n",
      " |-- MIN: double (nullable = true)\n",
      " |-- PRCP: double (nullable = true)\n",
      "\n",
      "Row(STATION=72532604894, Date='2016-04-24', Time='12:0:0 AM', LATITUDE=41.74278, LONGITUDE=-89.67611, ELEVATION=197.2, NAME='STERLING ROCK FALLS WHITESIDE CO JOSH BITTORF FIELD AIRPORT, IL US', TEMP=60.9, DEWP=48.4, SLP=1030.0, STP=999.9, VISIB=9.5, WDSP=5.3, MXSPD=9.9, GUST=15.0, MAX=80.1, MIN=44.6, PRCP=0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1097077"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noaaParquet.printSchema()\n",
    "print(noaaParquet.head())\n",
    "noaaParquet.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "We use a NoSQL data model, because of the very large NOAA data set. \n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "After cleaning the data, we will create a final table which can be queried for each months and it will return sun spot coverage and average weather for each site and globally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model. We start by aggregating the NOAA parquet data and averaging over the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "aggNOAAdata = noaaParquet.groupby('Date').agg( \n",
    "                                col('Date').cast('timestamp').alias('ts'),\n",
    "                                avg('TEMP').alias('avgTEMP'),\n",
    "                                avg('DEWP').alias('avgDEWP'), \n",
    "                                avg('SLP').alias('avgSLP'), \n",
    "                                avg('STP').alias('avgSTP'), \n",
    "                                avg('VISIB').alias('avgVISIB'), \n",
    "                                avg('WDSP').alias('avgWDSP'), \n",
    "                                avg('MXSPD').alias('avgMXSPD'), \n",
    "                                avg('GUST').alias('avgGUST'), \n",
    "                                avg('MAX').alias('avgMAX'), \n",
    "                                avg('MIN').alias('avgMIN'), \n",
    "                                avg('PRCP').alias('avgPRCP'), \n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date='2016-08-17', ts=datetime.datetime(2016, 8, 17, 0, 0), avgTEMP=71.27573149741825, avgDEWP=74.31652323580032, avgSLP=1014.8262478485369, avgSTP=603.6166092943199, avgVISIB=11.279948364888126, avgWDSP=6.893631669535283, avgMXSPD=14.56196213425129, avgGUST=22.15275387263339, avgMAX=82.72185886402751, avgMIN=60.264802065404474, avgPRCP=5.053485370051633)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggNOAAdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1096"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggNOAAdata.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We now joint the NOAA and sunspot data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfSpotWeather = spotsTable.join(aggNOAAdata, \n",
    "                (spotsTable.Date == aggNOAAdata.Date) ).drop( aggNOAAdata.Date ).drop(spotsTable.Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- spotId: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- SunspotNumber: double (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- avgTEMP: double (nullable = true)\n",
      " |-- avgDEWP: double (nullable = true)\n",
      " |-- avgSLP: double (nullable = true)\n",
      " |-- avgSTP: double (nullable = true)\n",
      " |-- avgVISIB: double (nullable = true)\n",
      " |-- avgWDSP: double (nullable = true)\n",
      " |-- avgMXSPD: double (nullable = true)\n",
      " |-- avgGUST: double (nullable = true)\n",
      " |-- avgMAX: double (nullable = true)\n",
      " |-- avgMIN: double (nullable = true)\n",
      " |-- avgPRCP: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSpotWeather.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We check that we have no NULL elements in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that key fields have valid values (no nulls or empty)\n",
    "dfSpotWeather.createOrReplaceTempView(\"SpotWeatherTable\")\n",
    "SpotWeather_check = spark.sql(\"\"\"\n",
    "    SELECT  COUNT(*)\n",
    "    FROM SpotWeatherTable\n",
    "    WHERE Date IS NULL OR SunSpotNumber IS NULL \n",
    "\"\"\")\n",
    "SpotWeather_check.show(1)\n",
    "SpotWeather_check.collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "spotId: Spot identifier\n",
    "Date: Date of observatoins\n",
    "SunspotNumber: number of spots\n",
    "ts: Date as timestamp\n",
    "avgTEMP: average temperature\n",
    "avgDEWP: average dew point\n",
    "avgSLP: average sealevel pressure\n",
    "avgSTP: average site pressure\n",
    "avgVISIB: average visibility\n",
    "avgWDSP: average windspeed\n",
    "avgMXSPD: average max. windspeed\n",
    "avgGUST: average wind gust\n",
    "avgMAX: average max. temp.\n",
    "avgMIN: average min. temp.\n",
    "avgPRCP: average precitipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Given the already large data set Spark is very usuful to deal with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The data should be updated on a yearly basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I choose NoSQL as data model which is good for linear scalbility. Increasing the data set posese no problems. The Spark framework allows access for hundreds of people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
